{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39763,"databundleVersionId":11756775,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom pathlib import Path\nimport csv\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T00:52:20.791451Z","iopub.execute_input":"2025-04-28T00:52:20.791621Z","iopub.status.idle":"2025-04-28T00:52:26.365822Z","shell.execute_reply.started":"2025-04-28T00:52:20.791605Z","shell.execute_reply":"2025-04-28T00:52:26.365087Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"all_inputs = [\n    f\n    for f in\n    Path('/kaggle/input/waveform-inversion/train_samples').rglob('*.npy')\n    if ('seis' in f.stem) or ('data' in f.stem)\n]\n\ndef inputs_files_to_output_files(input_files):\n    return [\n        Path(str(f).replace('seis', 'vel').replace('data', 'model'))\n        for f in input_files\n    ]\n\nall_outputs = inputs_files_to_output_files(all_inputs)\n\nassert all(f.exists() for f in all_outputs)\n\ntrain_inputs = [all_inputs[i] for i in range(0, len(all_inputs), 2)] # Sample every two\nvalid_inputs = [f for f in all_inputs if not f in train_inputs]\n\ntrain_outputs = inputs_files_to_output_files(train_inputs)\nvalid_outputs = inputs_files_to_output_files(valid_inputs)\n\nclass SeismicDataset(Dataset):\n    def __init__(self, inputs_files, output_files, n_examples_per_file=500):\n        assert len(inputs_files) == len(output_files)\n        self.inputs_files = inputs_files\n        self.output_files = output_files\n        self.n_examples_per_file = n_examples_per_file\n\n    def __len__(self):\n        return len(self.inputs_files) * self.n_examples_per_file\n\n    def __getitem__(self, idx):\n        # Calculate file offset and sample offset within file\n        file_idx = idx // self.n_examples_per_file\n        sample_idx = idx % self.n_examples_per_file\n\n        X = np.load(self.inputs_files[file_idx], mmap_mode='r')\n        y = np.load(self.output_files[file_idx], mmap_mode='r')\n\n        try:\n            return X[sample_idx].copy(), y[sample_idx].copy()\n        finally:\n            del X, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T00:52:26.366609Z","iopub.execute_input":"2025-04-28T00:52:26.366983Z","iopub.status.idle":"2025-04-28T00:52:26.464271Z","shell.execute_reply.started":"2025-04-28T00:52:26.366959Z","shell.execute_reply":"2025-04-28T00:52:26.463560Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dstrain = SeismicDataset(train_inputs, train_outputs)\ndsvalid = SeismicDataset(valid_inputs, valid_outputs)\n\ntrain_loader = DataLoader(dstrain, batch_size=16, shuffle=True, num_workers=4)\nval_loader = DataLoader(dsvalid, batch_size=16, shuffle=True, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T00:52:26.466107Z","iopub.execute_input":"2025-04-28T00:52:26.466345Z","iopub.status.idle":"2025-04-28T00:52:26.470851Z","shell.execute_reply.started":"2025-04-28T00:52:26.466328Z","shell.execute_reply":"2025-04-28T00:52:26.470206Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    # (Conv → BN → GELU) × 2\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_c,  out_c, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_c),\n            nn.GELU(),\n            nn.Conv2d(out_c, out_c, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_c),\n            nn.GELU()\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Down(nn.Module):\n    # Max-pool then DoubleConv\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_c, out_c)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Up(nn.Module):\n    # Upsample  →  concat with skip  →  DoubleConv\n    def __init__(self, in_c, out_c):\n        super().__init__()\n        # up-sample:        \n        self.up   = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2)\n        self.conv = DoubleConv(out_c * 3, out_c)   \n\n    def forward(self, x, skip):\n        x = self.up(x)                              \n        if x.shape[-2:] != skip.shape[-2:]:\n            skip = F.interpolate(skip, size=x.shape[-2:], mode=\"bilinear\",\n                                  align_corners=False)\n        x = torch.cat([skip, x], dim=1)             \n        return self.conv(x)\n\n# ---------- the network ---------- #\nclass UNet70(nn.Module):\n    def __init__(self, in_channels=5, base_c=64, out_scale=1000, out_shift=1500):\n        super().__init__()\n        self.scale  = out_scale\n        self.shift  = out_shift\n\n        # encoder\n        self.inc   = DoubleConv(in_channels,  base_c)        #  64\n        self.down1 = Down(base_c,        base_c * 2)         # 128\n        self.down2 = Down(base_c * 2,    base_c * 4)         # 256\n        self.down3 = Down(base_c * 4,    base_c * 8)         # 512\n        self.down4 = Down(base_c * 8,    base_c * 8)         # bottleneck\n\n        # decoder\n        self.up1 = Up(base_c * 8, base_c * 4)                # 256\n        self.up2 = Up(base_c * 4, base_c * 2)                # 128\n        self.up3 = Up(base_c * 2, base_c)                    #  64\n        self.up4 = Up(base_c,     base_c // 2)               #  32\n\n        # head\n        self.out_conv = nn.Conv2d(base_c // 2, 1, 1)         # 1 × H × W\n\n    def forward(self, x):\n        # ---------- encoder ----------\n        x1 = self.inc(x)     # N×64×H×W\n        x2 = self.down1(x1)  # N×128×H/2×W/2\n        x3 = self.down2(x2)  # N×256×H/4×W/4\n        x4 = self.down3(x3)  # N×512×H/8×W/8\n        x5 = self.down4(x4)  # N×512×H/16×W/16 (bottleneck)\n\n        # ---------- decoder ----------\n        y = self.up1(x5, x4)\n        y = self.up2(y,  x3)\n        y = self.up3(y,  x2)\n        y = self.up4(y,  x1)\n\n        y = self.out_conv(y)               # (N,1,H/2,W/2)\n\n        # enforce fixed 70×70 regardless of input size\n        y = F.interpolate(y, size=(70, 70), mode=\"bilinear\", align_corners=False)\n\n        return y * self.scale + self.shift  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T00:52:26.471766Z","iopub.execute_input":"2025-04-28T00:52:26.472031Z","iopub.status.idle":"2025-04-28T00:52:26.494043Z","shell.execute_reply.started":"2025-04-28T00:52:26.472007Z","shell.execute_reply":"2025-04-28T00:52:26.493312Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\n\nmodel = UNet70().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nloss_fn = nn.MSELoss()\n\nepochs = 20  \n\nbest_val_loss = float('inf')\n\nfor epoch in range(epochs):\n    # ── 1. TRAIN ───────────────────────────────────────────────────────\n    model.train()\n    running_loss = 0.0\n    for xb, yb in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}\"):\n        xb, yb = xb.to(device), yb.to(device).squeeze(1)\n        optimizer.zero_grad()\n        preds = model(xb).squeeze(1)\n        loss  = loss_fn(preds, yb)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    train_loss = running_loss / len(train_loader)\n\n    # ── 2. VALIDATE ────────────────────────────────────────────────────\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for xb, yb in val_loader:\n            xb, yb = xb.to(device), yb.to(device).squeeze(1)\n            preds  = model(xb).squeeze(1)\n            val_loss += loss_fn(preds, yb).item()\n\n    val_loss /= len(val_loader)\n    print(f\"Epoch {epoch+1} | train {train_loss:.4f} | val {val_loss:.4f}\")\n\n    # ── 3. CHECKPOINT ON VAL LOSS ─────────────────────────────────────\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), '/kaggle/working/best_model.pth')\n        print(f\"✅  New best model saved (val loss {best_val_loss:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T00:52:26.494777Z","iopub.execute_input":"2025-04-28T00:52:26.494996Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[Train] Epoch 1:  49%|████▊     | 152/313 [01:20<01:27,  1.85it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"%%time\ntest_files = list(Path('/kaggle/input/waveform-inversion/test').glob('*.npy'))\nlen(test_files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_cols = [f'x_{i}' for i in range(1, 70, 2)]\nfieldnames = ['oid_ypos'] + x_cols","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, test_files):\n        self.test_files = test_files\n\n\n    def __len__(self):\n        return len(self.test_files)\n\n\n    def __getitem__(self, i):\n        test_file = self.test_files[i]\n\n        return np.load(test_file), test_file.stem","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ds = TestDataset(test_files)\ndl = DataLoader(ds, batch_size=128, num_workers=4, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATH = \"/kaggle/working/best_model.pth\" \nmodel.eval()\nmodel.load_state_dict(torch.load(PATH, weights_only=True))\n\nwith open('submission.csv', 'wt', newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n    writer.writeheader()\n    \n    for inputs, oids_test in tqdm(dl, desc='test'):\n        inputs = inputs.to(device)\n        with torch.inference_mode():\n            outputs = model(inputs)\n\n        y_preds = outputs[:, 0].cpu().numpy()\n        \n        for y_pred, oid_test in zip(y_preds, oids_test):\n            for y_pos in range(70):\n                row = dict(\n                    zip(\n                        x_cols,\n                        [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]\n                    )\n                )\n                row['oid_ypos'] = f\"{oid_test}_y_{y_pos}\"\n            \n                writer.writerow(row)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}